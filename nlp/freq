#!/usr/bin/env python3

import sys
import os
import multiprocessing
import zipfile
from collections import Counter
import argparse
import datetime
import io

import requests
from bs4 import BeautifulSoup
from nltk.tokenize import RegexpTokenizer

try:
    import requests_cache
except ImportError:
    rcache = False
else:
    rcache = True


def eprint(s):
    print(s, file=sys.stderr)
    sys.stderr.flush()


def my_dir():
    return os.path.dirname(os.path.abspath(__file__))


def parse_args():
    parser = argparse.ArgumentParser()

    today = datetime.date.today().strftime('%Y%m%d')
    def_link = 'https://sjp.pl/slownik/odmiany/sjp-odm-%s.zip' % today

    dict_dir = os.path.join(my_dir(), '..', 'dictionary')

    parser.add_argument('-d', '--dict-url', default=def_link,
                        help='URL to a dictionary zip')
    parser.add_argument('-o', '--out',
                        default=os.path.join(dict_dir, 'words.xml'),
                        help='result file')
    parser.add_argument('--no-cache', action='store_true',
                        help="don't use cache for HTTP requests")

    return parser.parse_args()


def count_words(text, dct):
    t = RegexpTokenizer(r'[A-Za-zĄĆĘŁŃÓŚŻŹąćęłńóśżź]+')
    tokens =  t.tokenize(text)

    c = Counter()
    for tok in tokens:
        if tok.lower() in dct:
            c.update([tok.lower()])
        elif tok in dct:
            c.update([tok])
    return c


def site_text(addr):
    print("Fetching %s" % addr)
    sys.stdout.flush()

    try:
        r = requests.get(addr)
    except Exception as e:
        eprint('Fetching %s failed: %s' % (addr, str(e)))
        return ''

    soup = BeautifulSoup(r.text, 'html.parser')
    return soup.get_text()


def make_dict(zipf):
    '''Creates a dictionary of all Polish words (at least, according to sjp.pl),
    minus stopwords (which are returned separately). There are almost 4 million
    words in this dictionary, but most of these are useless to most users. A
    good dictionary should probably have ~200-300k of most common words, so
    we'll only use a dictionary to verify words correctness.'''
    words = set()
    delims = ', '
    for line in io.TextIOWrapper(zipf, 'utf-8'):
        line = line.strip()
        # exclude words without derivations (which are not conjugated or
        # declensed). These are mostly some latin phrases (for some reason
        # included in sjp.pl's dict)
        if ',' not in line and ' ' in line:
            continue

        for word in line.split(delims):
            # exclude very short words and words with non-alpha characters (like
            # "aa", "zzz", "quasi-sth", "i.t.p.",  etc.)
            if len(word) > 2 and word.isalpha():
                words.add(word.strip(delims))

    return words


def stopwords():
    fname = os.path.join(my_dir(), 'stopwords-pl.txt')
    words = set()
    with open(fname) as f:
        # one word per line
        for word in f:
            words.add(word.strip())
    return words


def get_dict(url):
    fname = os.path.join(my_dir(), '_odm_tmp.zip')
    if not os.path.exists(fname):
        print('Downloading %s... -> %s' % (url, fname))
        r = requests.get(url)

        if not r.ok:
            soup = BeautifulSoup(r.text, 'html.parser')
            eprint('Downloading dictionary from %s failed' % url)
            eprint(soup.get_text())
            sys.exit(1)

        with open(fname, 'wb') as f:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)

    with zipfile.ZipFile(fname) as z:
        print('Extracting  %s...' % fname)
        with z.open('odm.txt', 'rU') as odm:
            print('Creating a dictionary...')
            return make_dict(odm)


def read_my_dir(fname):
    path = os.path.join(my_dir(), fname)
    ret = []
    with open(path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                yield line


def sites():
    return list(read_my_dir('sites.txt'))


def exclude(results):
    excs = set(read_my_dir('excludes.txt'))
    for e in excs:
        try:
            del results[e]
        except KeyError:
            pass


def save_results(results, fname, *, min_occurences=1, top_score=None):
    if not top_score:
        top_score = results.most_common(1)[0][1]

    nmin, nmax = 1, 255
    groups = nmax - nmin
    group_capa = top_score / groups

    print('Writing results to %s' % fname)
    with open(fname, 'w') as f:
        f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
        f.write('<wordlist>\n')

        for word, occurences in results.most_common():
            if occurences < min_occurences:
                break;

            if occurences > top_score:
                occurences = top_score

            group = (occurences / round(group_capa)) + 1
            f.write('  <w f="%d">%s</w>\n' % (group, word))

        f.write('</wordlist>\n')


def main():
    args = parse_args()
    dct =  get_dict(args.dict_url)
    urls = sites()

    if not args.no_cache and rcache:
        print('HTTP cache will be used')
        requests_cache.install_cache('freq_http_cache')

    assert(dct)
    assert(urls)

    with multiprocessing.Pool(None) as pool:
        texts = pool.map(site_text, urls)

    # not in a process pool because of pickling speed... (and other issues, like
    # depending on fork() when using a global dct instance)
    print('Analyzing...')
    counts = [count_words(txt, dct) for txt in texts]

    gc = Counter()
    for c in counts:
        gc.update(c)

    # find first non-stopword word.
    sw = stopwords()
    mc = None
    for word, val in gc.most_common(len(sw) + 1):
        if word not in sw:
            mc = val
            break

    assert(mc)

    # include non-existing stopwords with low priority (are they still stopwords
    # though?)
    for word in sw:
        gc.setdefault(word, 100)

    exclude(gc)

    save_results(gc, args.out, min_occurences=5, top_score=mc)
    return 0

sys.exit(main())
